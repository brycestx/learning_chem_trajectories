## Introduction
_Variational Inference_ (VI) has played an important role in Bayesian model uncertainty calibration and in unsupervised representation learning.
It is different from _Markov Chain Monte Carlo_ (MCMC) methods, which rely on the Markov chain to reach an equilibrium distribution; in VI, one can easily draw i.i.d. samples from the variational distribution, and enjoy lower variance in inference.
However, VI is subject to inherent bias found in the data used to formulate the approximating variational distribution.

A major drawback of variational approximations is the inherent poor representation of statistical uncertainty, where uncertainty is often vastly underestimated [ref].
This overconfidence leads to poor inference of seminal statistics governing the unobserved variable, including the marginal likelihood of data or the posterior prediction in the Bayesian context.
This phenomenon is increasingly observed in amortized VI, where data representations are learned in a quasi-complete manner [ref].
Amortization can therefore lead to less exploration of different configurations of chemical representations during inference, and can increasingly bias and diminish convergence during model training.

The nature of the bias can be determined by the variational family in use, such as approximate methods like the factorial Gaussian, which can have detrimental effects on the expressiveness of the approximate posterior.
To ameliorate this realization, more rich families of distributions have been explored, however, optimization of more complex distributions suffer from convergence and numerical instability issues.
An example of this phenomenom can be demonstrated by rewriting the training objective of VI as the KL divergence, often called the variational free energy _F_, between the proposed configuration _q_ and the target distribution _f_:

\begin{equation} { F }  ( q ) = \mathbb { E } _ { q } [\log q ( z ) - \log f ( z ) ] = { D } _ { K L } ( q \| f ) \end{equation}

Upon calculation of KL divergence, _q_ receives a penalty when probability mass culminates in regions when _f_ has low density.
