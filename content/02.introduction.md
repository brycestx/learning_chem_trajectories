## Introduction
_Variational Inference_ (VI) has played an important role in Bayesian model uncertainty calibration and in unsupervised representation learning.
It is different from _Markov Chain Monte Carlo_ (MCMC) methods, which rely on the Markov chain to reach an equilibrium distribution; in VI, one can easily draw i.i.d. samples from the variational distribution, and enjoy lower variance in inference.
However, VI is subject to inherent bias found in the data used to formulate the approximating variational distribution.

A major drawback of variational approximations is the inherent poor representation of statistical uncertainty, where uncertainty is often vastly underestimated [ref].
This overconfidence leads to poor inference of seminal statistics governing the unobserved variable, including the marginal likelihood of data or the posterior prediction in the Bayesian context.
This phenomenon is increasingly observed in amortized VI, where data representations are learned in a quasi-complete manner [ref].
Amortization can therefore lead to less exploration of different configurations of chemical representations during inference, and can increasingly bias and diminish convergence during model training.

The nature of the bias can be determined by the variational family in use, such as approximate methods like the factorial Gaussian, which can have detrimental effects on the expressiveness of the approximate posterior.
To ameliorate this realization, more rich families of distributions have been explored, however, optimization of more complex distributions suffer from convergence and numerical instability issues.
An example of this phenomenon can be demonstrated by rewriting the training objective of VI as the KL divergence, often called the variational free energy _F_, between the proposed configuration _q_ and the target distribution _f_:

\begin{equation} { F }  ( q ) = \mathbb { E } _ { q } [\log q ( z ) - \log f ( z ) ] = { D } _ { K L } ( q \| f ) \end{equation}

Upon calculation of KL divergence, _q_ receives a penalty when probability mass culminates in regions when _f_ has low density.
The consequence of this objective leads to bias in the variational distribution which overestimates confidence; inhibiting the variational approximation from avoiding local minima, particularly when the posterior appears accurately represented.
This artifact is commonly observed when the target distribution is highly non-linear in structure, such as when the variational distribution only discovers a subset of modes from a multimodal distribution.

This work proposes two annealing schedules that are implemented to facilitate two orthogonal goals.
_Alpha-annealing_ strives to encourage latent space exploration of significant modes and prioritizes convergence towards an accurate representation of the posterior for inference.
_Beta-annealing_ is primarily concerned with reducing noise and regularization during training to learn an appropriate generative model that is not prone to convergence towards artificial minima.

*Alpha-annealing*: The optimization endeavor can often reach convergence issues due to that non-convex nature of the objective. One such way to mitigate this issue is to use energy tempering [Katahira et al., 2008; Abrol et al., 2014; Mandt et al., 2016]:

\begin{equation} \mathbb { E } _ { q } [\log q ( z ) - \log f ( z ) ] \end{equation}

where \alpha = \frac{1/T} and T is analogous to the temperature in statistical mechanics.
